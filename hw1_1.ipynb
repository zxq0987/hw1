{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5419514f-a7bc-4580-8e1c-23c738c33eb2",
   "metadata": {},
   "source": [
    "# a_tensor_initialization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7daf1283-fdaa-4f73-9155-368447f8c78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 1\n",
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "################################################## 2\n",
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.Tensor([1, 2, 3], device = 'cpu') #tensor 기본 속성 확인\n",
    "print(t1.dtype)\n",
    "print(t1.device)\n",
    "print(t1.requires_grad)\n",
    "print(t1.size())\n",
    "print(t1.shape)\n",
    "\n",
    "t1_cpu = t1.cpu()\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t2 = torch.Tensor([1, 2, 3], device = 'cpu')\n",
    "print(t2.dtype)\n",
    "print(t2.device)\n",
    "print(t2.requires_grad)\n",
    "print(t2.size())\n",
    "print(t2.shape)\n",
    "\n",
    "t2_cpu = t2.cpu()\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "a1 = torch.tensor(1) # 스칼라\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1]) # 벡터(1차원)\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]]) # 행렬(2차원)\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([[[1], [2]], [[3], [4]], [[5], [6]]]) # 3차원 텐서\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "a9 = torch.tensor([ # 5차원 텐서\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a11 = torch.tensor([\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a11.shape, a11.ndim)\n",
    "\n",
    "#a12 = torch.tensor([\n",
    "#    [[[1, 2, 3], [4, 5]]],\n",
    "#    [[[1, 2, 3], [4, 5]]],\n",
    "#    [[[1, 2, 3], [4, 5]]],\n",
    "#    [[[1, 2, 3], [4, 5]]],\n",
    "#])\n",
    "#print(a12.shape, a12.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4361d4f-67e2-4357-8a62-e42b6cadb9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a12 해결방법\n",
    "a12 = torch.tensor([\n",
    "    [[[1, 2, 3], [4, 5, 0]]],  # 0으로 패딩\n",
    "    [[[1, 2, 3], [4, 5, 0]]],\n",
    "    [[[1, 2, 3], [4, 5, 0]]],\n",
    "    [[[1, 2, 3], [4, 5, 0]]],\n",
    "])\n",
    "\n",
    "or\n",
    "\n",
    "a12 = torch.tensor([\n",
    "    [[[1, 2, 3], [4, 5, 6]]],  # 모두 길이 3으로\n",
    "    [[[1, 2, 3], [4, 5, 6]]],\n",
    "    [[[1, 2, 3], [4, 5, 6]]],\n",
    "    [[[1, 2, 3], [4, 5, 6]]],\n",
    "])\n",
    "\n",
    "or\n",
    "\n",
    "a12 = torch.tensor([\n",
    "    [[1, 2], [4, 5]],  # 모두 길이 2로\n",
    "    [[1, 2], [4, 5]],\n",
    "    [[1, 2], [4, 5]],\n",
    "    [[1, 2], [4, 5]],\n",
    "])\n",
    "\n",
    "텐서란?\n",
    ": 다차원 배열의 일반화된 형태\n",
    "\n",
    "어떨 때 쓰일까?\n",
    ": 대량 데이터를 효율적으로 처리해야 할 때"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a807e8f-90b3-495e-bae6-10c28e683a36",
   "metadata": {},
   "source": [
    "# b_tensor_initialization_copy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b8dc9c-6239-406f-8c26-1cf7b4ae9979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.Tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3) # 정수형\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "print(\"#\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fae8f4-6277-41da-b302-b324853162cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyTorch에서 텐서 생성 방식의 차이점을 보여주는 코드\n",
    "\n",
    "torch.Tensor() 무조건 float32로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e814d-5b44-4f4d-89cb-6ae87a280806",
   "metadata": {},
   "source": [
    "# c_tensor_initialization_constant_values.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d88a62a-fd23-4145-ac32-25c5dbe67eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size = (5,))\n",
    "t1_like = torch.ones_like(input = t1)\n",
    "print(t1)\n",
    "print(t1_like)\n",
    "\n",
    "t2 = torch.zeros(size = (6,))\n",
    "t2_like = torch.ones_like(input = t2)\n",
    "print(t2)\n",
    "print(t2_like)\n",
    "\n",
    "t3 = torch.zeros(size = (4,))\n",
    "t3_like = torch.ones_like(input = t3)\n",
    "print(t3)\n",
    "print(t3_like)\n",
    "\n",
    "t4 = torch.eye(n = 3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcaca25-ab63-42c7-b9b2-fd9f7445359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones()\n",
    ": 1로 채워진 텐서 생성\n",
    "\n",
    "torch.zeros()\n",
    ": 0으로 채워진 텐서 생성\n",
    "\n",
    "torch.eye()\n",
    ": 대각선은 1, 나머지는 0으로 채워진 텐서 생성\n",
    "\n",
    "torch.ones_like()\n",
    ": 템플릿 텐서의 모든 속성을 복사하되, 값만 1로 채워진 텐서 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288f0b51-5213-4e70-a0cb-bb3333da4ecc",
   "metadata": {},
   "source": [
    "# d_tensor_initialization_random_values.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9ab39b-1482-4f58-9e6a-392a0a8010db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 14]])\n",
      "tensor([[0.6070, 0.5958, 0.9599]])\n",
      "tensor([[ 0.3042, -0.4315,  1.0217]])\n",
      "tensor([[11.1650, 10.7828],\n",
      "        [ 9.4753, 10.0820],\n",
      "        [ 9.9322,  9.4959]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "##############################\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low = 10, high = 20, size = (1, 2)) # 10~19 사이 정수 랜덤\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand(size = (1, 3)) # 0~1 사이 균등분포\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.randn(size = (1, 3)) # 평균 0, 표준편차1 정규분포\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.normal(mean = 10.0, std = 1.0, size = (3, 2)) # 평균 10, 표준편차1 정규분포\n",
    "print(t4)\n",
    "\n",
    "t5 = torch.linspace(start = 0.0, end = 5.0, steps = 3) # 0부터 5까지 3개 구간으로 등분\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.arange(5) # 0부터 4까지 순차적 정수\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b4e7c-d8a9-466c-859d-f0a5ffd2a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed()\n",
    ": 제한된 랜덤값 얻음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b639f1-91e9-4ecf-b22e-b1bad0697d6f",
   "metadata": {},
   "source": [
    "# e_tensor_type_conversion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b33e206f-eb93-4b87-a250-9e3207bbb8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[13.0737,  6.8375,  1.3056],\n",
      "        [19.1222, 18.1333, 15.8734]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "<built-in method double of Tensor object at 0x0000023B5B557980>\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 3)) # 기본값: float32\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.ones((2, 3), dtype = torch.int16) # 16비트 정수로 지정\n",
    "print(b)\n",
    "\n",
    "c = torch.rand((2, 3), dtype = torch.float64) * 20 # 64비트 부동소수점\n",
    "print(c)\n",
    "\n",
    "d = b.to(torch.int32) # int16 → int32 변환\n",
    "print(d)\n",
    "\n",
    "double_d = torch.ones(10, 2, dtype = torch.double) # float32 → float64 변환\n",
    "short_e = torch.tensor([[1, 2]], dtype = torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).double() # .double() = float64로 변환\n",
    "short_e = torch.ones(10, 2).short() # .short() = int16으로 변환\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype = torch.short)\n",
    "\n",
    "print(double_d.double)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_f = torch.rand(5, dtype = torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d97f65d-adc3-4510-b5a7-5aa41fb31ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "서로 다른 dtype끼리 연산할 때의 결과\n",
    "-> 더 정밀한 타입으로 자동 변경\n",
    "\n",
    "float64 × int16 → float64\n",
    "float32 × int64 → float32\n",
    "int32 × int16 → int32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf82457-e4f0-4997-a30f-3888a3a19bd7",
   "metadata": {},
   "source": [
    "# f_tensor_operations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "809201c0-5f1d-4672-bdda-c0b4166433b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "##############################\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size = (2, 3))\n",
    "t2 = torch.ones(size = (2, 3))\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2 # 덧셈\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2 # 뺼셈\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2 # 곱셈\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2 # 나눗셈\n",
    "print(t9)\n",
    "print(t10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a78f46-1c41-4c3c-ad97-0105f529cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch에서도 산술연산자 사용 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7997ed8-867b-48d5-ba4c-20740fde791f",
   "metadata": {},
   "source": [
    "# g_tensor_operations_mm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d9ce24-ee57-4b15-ac1e-7ce7d7f71090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[-6.3439,  2.8569],\n",
      "        [ 0.2075, -1.0269]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "t1 = torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1])) # 벡터 내적\n",
    "print(t1, t1.size())\n",
    "\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3) # 일반 행렬 곱셈\n",
    "print(t4, t4.size())\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 3)\n",
    "t7 = torch.bmm(t5, t6) # 배치 행렬 곱셈\n",
    "print(t7.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be2bd3-4bf8-4182-adbd-e12c1aa62992",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot()\n",
    ": 1차원 벡터끼리만 가능\n",
    "\n",
    "mm()\n",
    ": 2차원 행렬끼리만 가능\n",
    "\n",
    "bmm()\n",
    ": 3차원 텐서끼리 각 배치별로 독립적으로 행렬 곱셈 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dee5eb-4492-4c89-982f-b6a1552bca01",
   "metadata": {},
   "source": [
    "# h_tensor_operations_matmul.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0bbafdd-faed-4218-bee2-b2332f6c3c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size()) # 벡터 × 벡터 → 내적\n",
    "\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size()) # 행렬 × 벡터 → 행렬-벡터 곱셈\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size()) # 배치행렬 × 벡터 → 브로드캐스팅\n",
    "\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size()) # 배치행렬 × 배치행렬 → 배치 행렬 곱셈\n",
    "\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size()) # 배치행렬 × 행렬 → 브로드캐스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022e238-895d-4650-b9a4-fe5e3d2d80dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "갑자기 든 생각. 행렬끼리 왜 곱하지? 어디서 쓰일 수 있지?\n",
    ": 신경망에서 핵심 역할, 뉴런의 작동 원리 자체가 행렬곱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4cf5de-024d-4018-af05-872447488204",
   "metadata": {},
   "source": [
    "# i_tensor_broadcasting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71452a0c-99b5-4a93-80e8-3225aa98a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n",
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "################################################## 6\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2) # 스칼라 브로드캐스팅\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4) # 벡터 프로드캐스팅\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)\n",
    "print(t5 - 2.0)\n",
    "print(t5 * 2.0)\n",
    "print(t5 / 2.0)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "def normalize(x):\n",
    "    return x / 255\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])\n",
    "t8 = torch.tensor([[3, 1]])\n",
    "t9 = torch.tensor([[5], [2]])\n",
    "t10 = torch.tensor([7])\n",
    "print(t7 + t8)\n",
    "print(t7 + t9)\n",
    "print(t8 + t9)\n",
    "print(t7 + t10)\n",
    "\n",
    "print(\"#\" * 50, 5)\n",
    "\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)\n",
    "print((t17 + t18).size())\n",
    "\n",
    "print(\"#\" * 50, 6)\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())\n",
    "\n",
    "#t25 = torch.empty(5, 2, 4, 1)\n",
    "#t26 = torch.empty(3, 1, 1)\n",
    "#print((t25 + t26).size())\n",
    "\n",
    "print(\"#\" * 50, 7)\n",
    "\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)\n",
    "\n",
    "t28 = torch.pow(t27, 2) # 거듭제곱\n",
    "print(t28)\n",
    "\n",
    "exp = torch.arange(1., 5.)\n",
    "a = torch.arange(1., 5.)\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3f74b-00f0-4860-ac9e-55727e336c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "브로드캐스팅이란?\n",
    "크기가 다른 배열(텐서)끼리 연산할 때, 자동으로 크기를 맞춰주는 메커니즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ae616-57b9-43f2-b5f4-f9ac458b3fbf",
   "metadata": {},
   "source": [
    "# j_tensor_indexing_slicing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deed8a98-f2a9-423e-8dc2-3a9bc921cd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "################################################## 1\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])\n",
    "print(x[:, 1])\n",
    "print(x[1, 2])\n",
    "print(x[: -1]) # 슬라이싱`\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "print(x[1:])\n",
    "print(x[1:, 3:])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "\n",
    "print(z[:2])\n",
    "print(z[1:, 1:3])\n",
    "print(z[:, 1:])\n",
    "\n",
    "z[1:, 1:3] = 0\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935de01-510f-45f7-a12f-b1fba286f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "슬라이싱은 어떨 때 쓰일까?\n",
    ": 이미지 처리, 자연어 처리, 데이터 분할, 배치 처리, 신경망 구조 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1af8c2-ca81-4d28-86fd-0c6fb8aa1f96",
   "metadata": {},
   "source": [
    "# k_tensor_reshaping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30fb38db-ceaa-47e3-a5b5-71de89e64464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n",
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)\n",
    "t3 = t1.reshape(1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "t7 = t6.squeeze()\n",
    "t8 = t6.squeeze(0) # 0번째 차원 제거\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "t10 = t9.unsqueeze(1) # 1번째 위치에 차원 추가\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t14 = t13.flatten() # 평면화\n",
    "print(t14)\n",
    "\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "t17 = torch.flatten(t15, start_dim = 1)\n",
    "print(t16)\n",
    "print(t17)\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)\n",
    "print(torch.permute(t18, (2, 0, 1)).size()) # 차원 순서 바꾸기\n",
    "\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t20 = torch.permute(t19, dims = (0, 1))\n",
    "t21 = torch.permute(t19, dims = (1, 0))\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "t22 = torch.transpose(t19, 0, 1)\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)\n",
    "print(t23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c224c-9921-4d67-bbff-cc5545c411a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "squeeze()\n",
    ": 크기가 1인 차원 제거\n",
    "\n",
    "unsqueeze()\n",
    ": 새로운 차원 추가\n",
    "\n",
    "flatten()\n",
    ": 다차원을 1차원으로\n",
    "\n",
    "permute()\n",
    ": 차원 순서 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1a74c-df2c-4e64-b750-e9b21ede17d0",
   "metadata": {},
   "source": [
    "# l_tensor_concat.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c17192d6-c6cf-4b21-8fde-4b54d45a3585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "################################################## 1\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "################################################## 2\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim = 1) # 서로 다른 크기 연결\n",
    "print(t4.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t5 = torch.arange(0, 3)\n",
    "t6 = torch.arange(3, 8)\n",
    "t7 = torch.cat((t5, t6), dim = 0)\n",
    "print(t7.shape) # 1차원 벡터 연결\n",
    "print(t7)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)\n",
    "t10 = torch.cat((t8, t9), dim = 0)\n",
    "print(t10.size()) # 2차원 행렬 연결\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim = 1)\n",
    "print(t11.size())\n",
    "print(t11)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim = 0)\n",
    "print(t15.size())\n",
    "print(t15)\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim = 1)\n",
    "print(t16.size())\n",
    "print(t16)\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)\n",
    "t19 = torch.cat((t17, t18), dim = 0)\n",
    "print(t19.size()) # 3차원 텐서 연결\n",
    "print(t19)\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim = 1)\n",
    "print(t20.size())\n",
    "print(t20)\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim = 2)\n",
    "print(t21.size())\n",
    "print(t21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962741b-86bf-427c-8d49-d85046156f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "텐서 합치는 건 어떨 때 쓰이지?\n",
    ": 배치 데이터 합치기, 피처 결합, 시계열 데이터 확장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba332c6f-a93e-4eeb-8236-5d604c622fda",
   "metadata": {},
   "source": [
    "# m_tensor_stacking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "369f481f-672d-447d-842a-d32b6a9a1772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "################################################## 1\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim = 0)\n",
    "t4 = torch.cat([t1.unsqueeze(dim = 0), t2.unsqueeze(dim = 0)], dim = 0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim = 1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim = 1), t2.unsqueeze(dim = 1)], dim = 1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim = 2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim = 2), t2.unsqueeze(dim = 2)], dim = 2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t9 = torch.arange(0, 3)\n",
    "t10 = torch.arange(3, 6)\n",
    "print(t9.size(), t10.size())\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim = 0)\n",
    "print(t11.size())\n",
    "print(t11)\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim = 0)\n",
    "print(t11.equal(t12))\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim = 1)\n",
    "print(t13.size())\n",
    "print(t13)\n",
    "\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim = 1)\n",
    "print(t13.equal(t14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174e06f-4091-46c1-9f54-bede4e8c8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack() vs torch.cat()\n",
    "\n",
    "torch.cat()\n",
    ": 기존 차원 확장\n",
    "\n",
    "torch.stack()\n",
    ": 새로운 차원 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c70cab-00df-4fda-8f1a-e5b6f6d46953",
   "metadata": {},
   "source": [
    "# n_tensor_vstack_hstack.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcc82fb2-9acd-4335-ab7f-98fe888eac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5)) # 세로로 쌓기\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "print(t9)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14)) # 세로로 쌓기\n",
    "print(t15)\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "print(t18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e66bd-ea0f-4d2d-a563-aac884dd4a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.vstack()\n",
    "\n",
    "t1 = [1, 2, 3]     ←── 첫 번째 줄\n",
    "t2 = [4, 5, 6]     ←── 두 번째 줄\n",
    "\n",
    "vstack 결과:\n",
    "[1, 2, 3]  ←── t1\n",
    "[4, 5, 6]  ←── t2\n",
    "\n",
    "torch.hstack()\n",
    "\n",
    "t1 = [1, 2, 3]  |  t2 = [4, 5, 6]\n",
    "                ↓\n",
    "hstack 결과: [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251703a-f4d0-4344-96c8-2acee3f38f47",
   "metadata": {},
   "source": [
    "## 숙제 후기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422777f9-46e3-4055-b254-888087815624",
   "metadata": {},
   "outputs": [],
   "source": [
    "지금까지 텐서가 무엇인지, 텐서를 어떻게 만드는지, 그리고 PyTorch에 내장된 다양한 함수들을 배웠습니다. \n",
    "특히 사칙연산과 행렬 계산 같은 여러 함수들을 직접 사용해 봤고,\n",
    "앞으로 텐서 코드를 작성할 때는 배운 함수들을 어떤 상황에 적용하면 좋을지 미리 생각하며 효율적으로 코드를 짤 수 있을 것 같습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
